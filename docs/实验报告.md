# 对联生成：序列标注 vs Seq2Seq 对比实验

## 一、任务分析

### 1.1 问题定义

给定上联序列 $X = (x_1, x_2, ..., x_n)$，生成对应的下联序列 $Y = (y_1, y_2, ..., y_n)$。对联任务的关键约束是**上下联等长**。

### 1.2 原始代码的建模方式

原始代码将对联生成建模为**序列标注任务**：

```
P(Y|X) = ∏ P(y_i | X)    # 每个位置独立预测
```

这种做法的**优点**是可以并行计算所有位置，训练速度快；**缺点**是忽略了下联内部的依赖关系（比如"风调雨顺"中"雨"的预测应该依赖于前面已经生成的"风调"）。

### 1.3 Seq2Seq的建模方式

Seq2Seq采用自回归生成：

```
P(Y|X) = ∏ P(y_i | y_1, ..., y_{i-1}, X)    # 考虑历史生成
```

理论上更符合语言生成的特性，但每个时间步必须等待前一步完成，无法并行。

### 1.4 本实验要回答的问题

1. 对于**等长序列生成**任务，Seq2Seq相比序列标注能带来多大提升？
2. 增加模型容量（层数、宽度）的边际收益是多少？
3. Attention机制在等长任务上是否有效？

---

## 二、数据集与预处理

### 2.1 数据集划分

原始数据共774,491条，按9:1划分训练集和验证集：

![数据集划分](../dataset/dataset_split.png)

**图1：数据集划分比例**

| 数据集 | 样本数 | 占比 |
|--------|--------|------|
| 训练集 | 693,442 | 89.5% |
| 验证集 | 77,049 | 9.9% |
| 测试集 | 4,000 | 0.5% |

### 2.2 长度分布分析

![长度分布](../dataset/length_distribution.png)

**图2：样本长度分布**

| 统计量 | 值 |
|--------|------|
| 平均长度 | 9.3字 |
| 中位数 | 7字 |
| 最大长度 | 32字 |

七言对联占比最高，这说明模型需要处理的序列长度较短（平均<10），这对Attention机制不利——Attention的优势在于长序列的长程依赖建模。

### 2.3 Seq2Seq所需的特殊Token

Seq2Seq模型需要额外的 `<SOS>` 和 `<EOS>` 标记来控制生成的开始和结束

---

## 三、实验设计

### 3.1 对比实验配置

为了控制变量，设计如下5组实验：

**表1：实验配置**

| 实验 | 框架 | RNN | 层数 | 隐藏维度 | 对比目的 |
|------|------|-----|------|----------|----------|
| Exp1 | 序列标注 | BiLSTM | 1 | 256 | 基线 |
| Exp2 | Seq2Seq | GRU | 1 | 256 | 框架对比（vs Exp1） |
| Exp3 | Seq2Seq | GRU | 2 | 256 | 层数影响（vs Exp2） |
| Exp4 | Seq2Seq | GRU | 2 | 512 | 宽度影响（vs Exp3） |
| Exp5 | Seq2Seq+Attn | GRU | 2 | 256 | Attention效果（vs Exp3） |

### 3.2 其他超参数（固定）

| 参数 | 值 | 说明 |
|------|-----|------|
| Embedding维度 | 128 | 词向量维度 |
| Batch Size | 768 | 受限于显存 |
| 学习率 | 0.001 | Adam优化器 |
| Epochs | 20 | 训练轮数 |
| Teacher Forcing | 50% | Seq2Seq训练策略 |

### 3.3 关键实现细节

**Seq2Seq模型**：
- Encoder：双向GRU，输出维度为 hidden_dim（两个方向拼接后再投影）
- Decoder：单向GRU，每步输入为上一步预测 + Attention上下文
- Attention：加性注意力（Bahdanau Attention）

**训练策略**：
- Teacher Forcing比例50%：一半时间用真实标签，一半时间用模型预测
- 这是为了缓解训练-推理不一致问题（Exposure Bias）

---

## 四、实验结果

### 4.1 Loss曲线

![Loss曲线](../figures/loss_curves.png)

**图3：训练/验证Loss曲线**

**关键观察**：
- BiLSTM（蓝色）的训练Loss始终高于Seq2Seq，但验证Loss最终最低（4.999）
- Seq2Seq模型的训练Loss下降更快（到epoch 10左右已经到3.9），但验证Loss在5.04-5.08之间
- 这说明Seq2Seq存在**过拟合倾向**：训练Loss低但泛化能力不如BiLSTM

### 4.2 指标对比

![指标对比](../figures/metrics_comparison.png)

**图4：BLEU和Rouge-L得分对比**

**表2：完整实验结果**

| 模型 | Val Loss | BLEU | Rouge-L | 时间/epoch |
|------|----------|------|---------|------------|
| Exp1: BiLSTM | **4.999** | 0.77% | 8.35% | 22s |
| Exp2: Seq2Seq L1 | 5.078 | 3.63% | 16.28% | 467s |
| Exp3: Seq2Seq L2 | 5.045 | 3.86% | 16.76% | 495s |
| Exp4: Seq2Seq L2 H512 | 5.080 | **4.03%** | **17.07%** | 694s |
| Exp5: Seq2Seq+Attn | 5.040 | 3.83% | 16.91% | 486s |

### 4.3 核心发现分析

#### 发现1：Seq2Seq的BLEU/Rouge-L显著高于BiLSTM

| 对比 | BLEU提升 | Rouge-L提升 |
|------|----------|-------------|
| Exp2 vs Exp1 | +2.86% (3.7倍) | +7.93% (1.9倍) |

**但Val Loss却更差**（5.078 vs 4.999），这看起来矛盾。原因是：
- Loss是逐token计算的交叉熵，BiLSTM在每个位置上预测得更"稳"
- BLEU/Rouge-L评估的是整体序列相似度，Seq2Seq生成的序列整体更连贯

#### 发现2：增加容量的边际收益递减

![消融实验](../figures/ablation_study.png)

**图5：层数和隐藏维度的影响**

| 改动 | BLEU变化 | 时间成本 | 性价比 |
|------|----------|----------|--------|
| 1层→2层 | +0.23% | +28s/epoch | 一般 |
| 256→512 | +0.17% | +199s/epoch | 差 |

增加隐藏维度的收益很小（BLEU +0.17%），但训练时间增加40%，**不划算**。

#### 发现3：Attention无效

Exp3 vs Exp5（唯一区别是有无Attention）：

| 指标 | Exp3 (无Attn) | Exp5 (有Attn) | 差异 |
|------|---------------|---------------|------|
| BLEU | 3.86% | 3.83% | -0.03% |
| Rouge-L | 16.76% | 16.91% | +0.15% |

几乎没有提升。**原因分析**：
1. 对联上下联**严格等长**，位置天然对齐，不需要Attention来学习对齐
2. 序列很短（平均9字），即使没有Attention，Decoder的隐藏状态也能记住全部信息

### 4.4 训练效率

| 模型 | 时间/epoch | 20 epochs总时间 | 相对BiLSTM |
|------|------------|-----------------|------------|
| BiLSTM | 22s | 7分钟 | 1x |
| Seq2Seq L1 | 467s | 2.6小时 | **21x** |
| Seq2Seq L2 H512 | 694s | 3.9小时 | **32x** |

Seq2Seq慢的原因是**自回归无法并行**：生成第i个字必须等第i-1个字生成完。而BiLSTM可以一次并行处理整个序列

---

## 五、生成样例分析

**表3：生成样例对比**

| 上联 | BiLSTM | Seq2Seq | 分析 |
|------|--------|---------|------|
| 笑取琴书温旧梦 | 喜来月墨醉新情 | 邀来翰墨赋新春 | Seq2Seq的"翰墨"对"琴书"更工整 |
| 我有诗情堪纵酒 | 谁无酒意可留诗 | 好凭墨意不成诗 | BiLSTM保持了"酒-诗"的呼应 |
| 我以真诚溶冷血 | 人而大梦醉清心 | 凭之大爱有高风 | Seq2Seq的"大爱-高风"语义更通顺 |
| 马齿草焉无马齿 | 羊头花也有羊心 | 羊毫花岂有牛头 | 都没能理解"马齿"的双关 |

**定性观察**：
- Seq2Seq生成的下联**整体连贯性更好**，这和BLEU/Rouge-L指标一致
- BiLSTM有时会生成语法正确但**语义不通**的句子（如"人而大梦"）
- 两者都无法真正理解对联的文化内涵（如双关、典故）

---

## 六、遇到的问题

### 问题1：Seq2Seq生成重复

**现象**：生成出现 "今古今古今" 这样的重复

**原因**：自回归解码容易陷入循环，尤其在Teacher Forcing训练后

**解决**：添加repetition_penalty=1.5，对已生成的token降低概率

### 问题2：标点位置错乱

**现象**：上联标点在位置5，下联标点在位置7

**原因**：Seq2Seq不强制等长，标点可能被"挤"到其他位置

**解决**：强制标点位置与上联对齐

---

## 七、结论

### 回答开头提出的三个问题

**Q1: Seq2Seq vs 序列标注，哪个更好？**

指标上Seq2Seq更好（BLEU 3.63% vs 0.77%），但代价是训练时间增加21倍。对于**等长生成**任务，序列标注的性价比更高。

**Q2: 增加模型容量的边际收益？**

收益递减。1层→2层带来+0.23% BLEU，256→512仅+0.17% BLEU，但时间成本分别增加6%和40%。

**Q3: Attention有用吗？**

没用。对联任务位置天然对齐，序列又短，Attention没有发挥空间。

### 一句话总结

> 对联生成这种**等长、短序列**任务，序列标注比Seq2Seq更合适；Seq2Seq的优势在于**变长、长序列**生成（如机器翻译）。

---

## 附录：代码修改清单

| 文件 | 修改内容 |
|------|----------|
| `preprocess.py` | 添加训练集/验证集划分 |
| `module/tokenizer.py` | 添加SOS/EOS token |
| `module/seq2seq.py` | 新增：GRUEncoder, GRUDecoder, Attention, Seq2SeqModel |
| `main.py` | 支持Seq2Seq训练、验证集评估、history.json保存 |
