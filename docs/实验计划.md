根据代码分析，当前项目是**序列标注**模式（输入输出等长，一一对应），需要改为**Seq2Seq自回归框架**（Encoder-Decoder结构，Decoder逐步生成）。

---

# 实施计划

## 第一阶段：代码修改

### 1. 数据预处理修改
- **修改 [preprocess.py](cci:7://file:///sda/data/yaoxianglin/CoupletAI/preprocess.py:0:0-0:0)**：添加训练集/验证集划分逻辑（如 9:1 划分）
- **添加特殊token**：`<SOS>`（Start of Sequence）、`<EOS>`（End of Sequence）用于Decoder

### 2. 实现 Seq2Seq 模型
- **新建 `module/seq2seq.py`**：
  - `GRUEncoder`：使用GRU替代LSTM，支持多层、双向
  - `GRUDecoder`：带Attention机制的GRU解码器
  - `Seq2SeqModel`：整合Encoder+Decoder的完整模型
- **对比实验模型**：
  - 不同层数（1层 vs 2层 vs 3层）
  - 不同隐藏层宽度（128 vs 256 vs 512）
  - LSTM vs GRU对比

### 3. 训练逻辑修改
- **修改 [main.py](cci:7://file:///sda/data/yaoxianglin/CoupletAI/main.py:0:0-0:0)**：
  - 支持Seq2Seq训练（Teacher Forcing）
  - 添加验证集评估
  - 记录训练/验证loss用于绘图
  - 保存loss日志到文件

---

## 第二阶段：实验设计（体现工作量）

| 实验编号 | 模型 | RNN类型 | 层数 | 隐藏维度 | 说明 |
|---------|------|--------|------|---------|------|
| Exp1 | Baseline | BiLSTM | 1 | 256 | 原始序列标注模型 |
| Exp2 | Seq2Seq | GRU | 1 | 256 | 基础Seq2Seq |
| Exp3 | Seq2Seq | GRU | 2 | 256 | 增加层数 |
| Exp4 | Seq2Seq | GRU | 2 | 512 | 增加宽度 |
| Exp5 | Seq2Seq+Attn | GRU | 2 | 256 | 加Attention |

---

## 第三阶段：实验报告（图表丰富）

### 报告结构建议
1. **引言**：问题背景、任务描述
2. **相关工作**：简述Seq2Seq、Attention机制
3. **方法**：
   - 原始代码分析（序列标注 vs Seq2Seq对比图）
   - 改进方案（Encoder-Decoder架构图）
   - 模型结构图（手绘风格或简洁框图，避免AI味）
4. **实验**：
   - 数据集划分说明（饼图：训练集/验证集/测试集）
   - 超参数表格
   - **Loss曲线图**：训练loss + 验证loss随epoch变化（matplotlib绘制）
   - **对比实验柱状图**：不同模型BLEU/Rouge-L对比
   - **消融实验**：层数/宽度影响的折线图
5. **案例展示**：
   - 输入上联 → 模型输出下联 的表格展示
   - 对比不同模型的生成结果
6. **结论与思考**

### 建议的图表清单（~8张图）
| 图表 | 类型 | 内容 |
|-----|------|-----|
| 图1 | 架构图 | Seq2Seq模型结构（Encoder-Decoder-Attention） |
| 图2 | 饼图 | 数据集划分比例 |
| 图3 | 折线图 | 训练/验证Loss曲线（多模型对比） |
| 图4 | 柱状图 | 各模型BLEU得分对比 |
| 图5 | 柱状图 | 各模型Rouge-L得分对比 |
| 图6 | 折线图 | 层数对性能的影响 |
| 图7 | 折线图 | 隐藏维度对性能的影响 |
| 表1 | 表格 | 超参数配置表 |
| 表2 | 表格 | 生成样例展示 |

---

## 执行顺序

1. **修改数据预处理** → 划分验证集 + 添加SOS/EOS
2. **实现Seq2Seq模型** → GRU Encoder + Attention Decoder
3. **修改训练脚本** → 支持Seq2Seq训练 + 日志记录
4. **跑实验** → 5组对比实验
5. **绘制图表** → 用matplotlib生成图表
6. **撰写报告** → 整合图表和分析

---

## 待办列表

- [x] 1.1 修改 `tokenizer.py` 添加 SOS/EOS token
- [x] 1.2 修改 `preprocess.py` 划分验证集
- [x] 1.3 生成数据集划分饼图 (`dataset/dataset_split.png`)
- [x] 1.4 生成样本长度分布图 (`dataset/length_distribution.png`)
- [x] 2.1 创建 `module/seq2seq.py` - GRUEncoder
- [x] 2.2 创建 `module/seq2seq.py` - Attention机制
- [x] 2.3 创建 `module/seq2seq.py` - GRUDecoder
- [x] 2.4 创建 `module/seq2seq.py` - Seq2SeqModel
- [x] 3.1 修改 `main.py` 支持Seq2Seq训练
- [x] 3.2 添加验证集评估逻辑
- [x] 3.3 保存训练/验证loss日志
- [ ] 4.1 运行 Exp1: Baseline BiLSTM
- [ ] 4.2 运行 Exp2: Seq2Seq GRU 1层
- [ ] 4.3 运行 Exp3: Seq2Seq GRU 2层
- [ ] 4.4 运行 Exp4: Seq2Seq GRU 2层 512维
- [ ] 4.5 运行 Exp5: Seq2Seq+Attn
- [ ] 5.1 绘制Loss曲线对比图
- [ ] 5.2 绘制BLEU/Rouge-L对比柱状图
- [ ] 5.3 绘制消融实验图
- [ ] 6.1 撰写实验报告

---

## 发现的问题与优化

### 问题1: Seq2Seq生成质量差
**现象**：Seq2Seq模型虽然BLEU指标较高，但生成的对联不成句，出现大量重复字符（如"今古今古今"）和标点符号。

**原因分析**：
1. **训练-推理不一致**：训练时使用Teacher Forcing（50%概率看真实答案），推理时完全自回归，分布差异大
2. **错误累积**：自回归解码一旦出错，后续生成会持续受影响
3. **任务不匹配**：对联上下联等长，序列标注天然适合，Seq2Seq的优势在于变长序列生成

**优化措施**：
- [x] 添加重复惩罚（repetition_penalty=1.5）抑制重复生成
- [x] 添加标点符号位置对齐（punctuation_ids）：上联是标点则下联强制相同标点，上联非标点则下联禁止标点
- [x] 强制等长生成（force_same_length=True）保证字数一致

### 问题2: Seq2Seq训练慢
**现象**：BiLSTM约22秒/epoch，Seq2Seq约460秒/epoch（慢20倍）

**原因**：Seq2Seq是自回归的，每个时间步需要顺序计算；而BiLSTM序列标注可以并行处理整个序列。

**优化建议**：
- 使用多卡训练（DataParallel）加速
- 减小batch_size，增加梯度累积

### 问题3: 多卡训练兼容性
多卡训练的模型保存时会有`module.`前缀，单卡加载需要处理。当前代码已兼容。

---

## 实验结论（待填写）

| 模型 | Val Loss | BLEU | Rouge-L | 生成质量 |
|------|----------|------|---------|---------|
| BiLSTM | | | | |
| Seq2Seq L1 | | | | |
| Seq2Seq L2 | | | | |
| Seq2Seq L2 H512 | | | | |
| Seq2Seq+Attn | | | | |

**核心发现**：对于对联生成这种等长序列任务，序列标注方法优于Seq2Seq自回归方法。